# Estratégia de Testes e Cobertura de Código

## Visão Geral

Este documento descreve a estratégia completa de testes para o projeto de análise e predição de desenvolvimento educacional da Associação Passos Mágicos, garantindo 100% de cobertura nos módulos críticos e 90% de cobertura geral do projeto.

---

## 1. Objetivo e Métricas

### Objetivos
- ✓ 100% de cobertura dos módulos principais (lstm_model, utils, evaluate)
- ✓ 90% de cobertura geral do projeto
- ✓ Testes independentes, determinísticos e rápidos
- ✓ Validação automática em CI/CD antes de merge

### Métricas de Sucesso
- Todos os testes passam na CI/CD
- Coverage >= 90% (obrigatório)
- Zero warnings de linter
- Type checking com mypy sem erros
- Performance aceitável (< 5 minutos para suite completa)

---

## 2. Estrutura de Testes

### Organização de Pastas

```
tests/
├── conftest.py              # Configuração global do pytest
├── test_lstm_model.py       # Testes do modelo LSTM (100%)
├── test_utils.py            # Testes de utilitários (100%)
├── test_evaluate.py         # Testes de avaliação (100%)
├── test_preprocessing.py    # Testes de pré-processamento
├── test_data_loader.py      # Testes de carregamento de dados
├── test_config.py           # Testes de configuração
├── test_main.py             # Testes da API FastAPI
├── test_lifespan.py         # Testes de ciclo de vida
├── test_audit_route.py      # Testes de rotas de auditoria
├── test_reproducibility.py  # Testes de reprodutibilidade
├── test_train_integration.py    # Testes de integração
├── test_train_route_coverage.py # Testes de cobertura da rota
└── test_train_unit.py       # Testes unitários de treino
```

### Convenções de Nomenclatura

```python
# Arquivo de teste
test_<módulo>.py

# Classe de teste
class Test<Funcionalidade>:
    pass

# Método de teste
def test_<função>_<cenário>(self):
    pass

# Exemplos
test_lstm_model.py
class TestLSTMModelInit:
    def test_init_default_parameters(self):
        pass
```

---

## 3. Cobertura por Módulo

### 3.1 lstm_model.py (100%)

**Classe**: `LSTMModel(nn.Module)`

**Testes Implementados**:
- Inicialização
  - [ ] Com parâmetros padrão
  - [ ] Com parâmetros customizados
  - [ ] Verificar configuração LSTM batch_first

- Forward Pass
  - [ ] Shapes de entrada/saída corretos
  - [ ] Diferentes dimensões
  - [ ] Gradientes para backpropagation
  - [ ] Ausência de NaN

- Representação String
  - [ ] __str__ retorna string
  - [ ] Contém informações do modelo

- Compatibilidade
  - [ ] CPU
  - [ ] CUDA (se disponível)
  - [ ] State dict save/load

- Edge Cases
  - [ ] Zero input
  - [ ] Valores muito pequenos
  - [ ] Valores muito grandes
  - [ ] Sequência mínima

**Cobertura Obtida**: 100%

```python
# Exemplo de teste
def test_forward_default_shapes(self):
    """Testa o forward pass com shapes padrão."""
    model = LSTMModel()
    batch_size, seq_length, input_size = 32, 10, 1
    
    x = torch.randn(batch_size, seq_length, input_size)
    output = model.forward(x)
    
    assert output.shape == (batch_size, 1)
```

### 3.2 utils.py (100%)

**Funções**: `save_model`, `load_model`

**Testes Implementados**:
- save_model
  - [ ] Cria arquivo
  - [ ] Arquivo não vazio
  - [ ] Caminho padrão
  - [ ] Sobrescreve existente
  - [ ] Diferentes arquiteturas

- load_model
  - [ ] Retorna modelo
  - [ ] Pesos correspondem
  - [ ] Modo eval ativado
  - [ ] Arquivo inexistente (erro)
  - [ ] Arquivo corrompido (erro)
  - [ ] Arquitetura diferente (erro)

- Integração
  - [ ] Ciclo save/load completo
  - [ ] Múltiplos ciclos
  - [ ] Save/load com inferência

**Cobertura Obtida**: 100%

```python
# Exemplo de teste
def test_save_load_cycle(self):
    """Testa um ciclo completo de save e load."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = os.path.join(tmpdir, "model.pth")
        
        original_model = LSTMModel()
        save_model(original_model, path)
        
        loaded_model = LSTMModel()
        load_model(loaded_model, path)
        
        # Verificar igualdade
        x = torch.randn(32, 10, 1)
        assert torch.allclose(
            original_model(x),
            loaded_model(x)
        )
```

### 3.3 evaluate.py (100%)

**Funções**: `evaluate_model`, `calculate_metrics`

**Testes Implementados**:
- calculate_metrics
  - [ ] Retorna dicionário
  - [ ] Chaves corretas (mae, rmse, mape)
  - [ ] Valores são floats
  - [ ] Cálculos corretos
  - [ ] Predição perfeita (0)
  - [ ] Valores grandes
  - [ ] Valor único
  - [ ] Valores negativos

- evaluate_model
  - [ ] Retorna tupla
  - [ ] Arrays numpy
  - [ ] Shapes correspondem
  - [ ] Sem NaN
  - [ ] Modo eval
  - [ ] Múltiplos lotes
  - [ ] Batch size único
  - [ ] Sem gradientes

- Integração
  - [ ] Métricas de avaliação
  - [ ] Valores razoáveis
  - [ ] RMSE >= MAE

**Cobertura Obtida**: 100%

```python
# Exemplo de teste
def test_calculate_metrics_mae_calculation(self):
    """Testa o cálculo correto do MAE."""
    predictions = np.array([1.0, 2.0, 3.0])
    actuals = np.array([1.5, 2.5, 3.5])
    
    result = calculate_metrics(predictions, actuals)
    expected_mae = np.mean(np.abs(predictions - actuals))
    
    assert np.isclose(result['mae'], expected_mae)
```

### 3.4 data_loader.py (~90%)

**Funções**: `load_stock_data`, `validate_symbol`

**Testes Implementados**:
- load_stock_data
  - [x] Carregamento bem-sucedido de dados
  - [x] Tratamento de símbolos inválidos
  - [x] Validação de intervalo de datas
  - [x] Tratamento de erros de conexão
  - [x] Formatação de dados retornados

- validate_symbol
  - [x] Símbolos válidos
  - [x] Símbolos inválidos
  - [x] Casos especiais

**Cobertura Obtida**: ~90%

### 3.5 seed_manager.py (~95%)

**Funções**: `set_seed`, `get_seed`, `ensure_reproducibility`

**Testes Implementados**:
- set_seed
  - [x] Define seed para PyTorch
  - [x] Define seed para NumPy
  - [x] Define seed para Python random
  - [x] Configurações de CUDA

- ensure_reproducibility
  - [x] Garante resultados determinísticos
  - [x] Verificação de consistência
  - [x] Repetição de resultados

**Cobertura Obtida**: ~95%

### 3.6 train.py (~90%)

**Classes/Funções**: `ModelTrainer`, `run_training_pipeline`

**Testes Implementados**:
- ModelTrainer.__init__
  - [x] Inicializa corretamente
  - [x] MSELoss como critério
  - [x] Adam como otimizador
  - [x] Device management

- ModelTrainer.train
  - [x] Executa treinamento
  - [x] Retorna loss_history
  - [x] Loss diminui ao longo do tempo
  - [x] Modo train ativado

- run_training_pipeline
  - [x] Executa pipeline completo
  - [x] Retorna métricas
  - [x] Salva artefatos
  - [x] Logging MLflow

**Cobertura Obtida**: ~90%

---

## 4. Estratégia de Testes

### 4.1 Estrutura AAA (Arrange-Act-Assert)

Todos os testes seguem este padrão:

```python
def test_something(self):
    # ARRANGE: Preparar dados
    model = LSTMModel()
    x = torch.randn(32, 10, 1)
    expected_shape = (32, 1)
    
    # ACT: Executar código sob teste
    output = model.forward(x)
    
    # ASSERT: Verificar resultado
    assert output.shape == expected_shape
```

### 4.2 Tipos de Teste

#### Unit Tests
Testam funções/métodos isoladamente
```python
@pytest.mark.unit
def test_calculate_metrics_mae(self):
    """Testa cálculo isolado de MAE"""
```

#### Integration Tests
Testam integração entre componentes
```python
def test_evaluate_and_calculate_integration(self):
    """Testa avaliação + cálculo de métricas"""
```

#### Edge Case Tests
Testam limites e casos extremos
```python
def test_zero_input(self):
    """Testa com entrada zero"""
    
def test_very_large_input(self):
    """Testa com valores muito grandes"""
```

### 4.3 Fixtures

Fixtures reutilizáveis em `conftest.py`:

```python
@pytest.fixture
def lstm_model():
    """Modelo LSTM padrão"""
    return LSTMModel()

@pytest.fixture
def pytorch_device():
    """Device apropriado (CPU/CUDA)"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

@pytest.fixture
def minmax_scaler():
    """Scaler normalizado"""
    scaler = MinMaxScaler()
    scaler.fit(np.array([[0], [1]]))
    return scaler
```

---

## 5. Execução de Testes

### 5.1 Instalação

```bash
# Dependências de teste
pip install -r requirements-dev.txt

# Ou individuais
pip install pytest pytest-cov pytest-xdist torch numpy
```

### 5.2 Comando Básico

```bash
# Rodar todos os testes
pytest tests/ -v

# Com cobertura
pytest tests/ --cov=src --cov-report=term-missing -v

# Em paralelo (rápido)
pytest tests/ -n auto -v

# Teste específico
pytest tests/test_lstm_model.py::TestLSTMModelInit::test_init_default_parameters -v
```

### 5.3 Com Make

```bash
# Ver todos os comandos
make help

# Rodar testes
make test

# Com cobertura
make coverage

# HTML report
make coverage-html

# Todos os checks
make quality
```

### 5.4 Verificar Coverage

```bash
# Terminal report
pytest --cov=src --cov-report=term-missing

# HTML report
pytest --cov=src --cov-report=html
# Abrir: htmlcov/index.html

# Verificar threshold
coverage report --fail-under=90
```

---

## 6. CI/CD Integration

### 6.1 GitHub Actions Pipeline

Arquivo: `.github/workflows/ci-cd-pipeline.yml`

**Jobs**:
1. **code-quality** - Lint, format, type check
2. **build** - Verificar sintaxe e imports
3. **tests** - Testes unitários + coverage >= 90%
4. **integration-tests** - Testes de integração
5. **train-model** - Treina modelo (main branch)
6. **security** - Análise de segurança
7. **documentation** - Verifica docs
8. **report** - Gera relatório

### 6.2 Triggering

Pipeline executa em:
- `push` para `main` ou `develop`
- `pull_request` para `main` ou `develop`

### 6.3 Requisitos para Merge

- ✓ Todos os testes passam
- ✓ Coverage >= 90%
- ✓ Sem warnings de lint
- ✓ Type checking ok
- ✓ Documentação atualizada

---

## 7. Qualidade de Código

### 7.1 Linting

```bash
# Pylint
pylint src/ app/ --exit-zero

# Flake8
flake8 src/ app/ --max-line-length=100

# Com Make
make lint
```

### 7.2 Formatação

```bash
# Black
black src/ app/ tests/

# isort
isort src/ app/ tests/

# Com Make
make format
```

### 7.3 Type Checking

```bash
# MyPy
mypy src/ app/ --exit-zero

# Com Make
make type-check
```

### 7.4 Security

```bash
# Bandit
bandit -r src/ app/ -v

# Com Make
make security
```

### 7.5 Todos os Checks

```bash
# Com Make (recomendado)
make quality

# Manual
pytest tests/ --cov=src --cov-report=term-missing
pylint src/ app/ --exit-zero
black --check src/ app/
mypy src/ app/ --exit-zero
flake8 src/ app/ --exit-zero
bandit -r src/ app/ -v
```

---

## 8. Boas Práticas

### ✓ Fazer
- [x] Usar fixtures para dados compartilhados
- [x] Testes independentes
- [x] Testes determinísticos
- [x] Nomes descritivos
- [x] Assert com mensagens
- [x] Um assert por teste (ou relacionados)
- [x] Testar casos extremos
- [x] Mockar dependências externas

### ✗ Evitar
- [ ] Testes interdependentes
- [ ] Testes com side effects
- [ ] Múltiplas responsabilidades
- [ ] Testes muito lentos
- [ ] Hardcoded paths
- [ ] Print para debug (usar logging)

### Exemplo Bom

```python
def test_model_forward_with_batch(self):
    """Testa forward pass com batch size 32."""
    # Arrange
    model = LSTMModel()
    batch_size = 32
    x = torch.randn(batch_size, 10, 1)
    
    # Act
    output = model.forward(x)
    
    # Assert
    assert output.shape == (batch_size, 1), \
        f"Expected shape ({batch_size}, 1), got {output.shape}"
```

---

## 9. Troubleshooting

### Problema: Module not found
```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
pytest tests/
```

### Problema: CUDA not available
```python
@pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="CUDA not available"
)
def test_cuda():
    pass
```

### Problema: Testes lentos
```bash
# Rodar em paralelo
pip install pytest-xdist
pytest tests/ -n auto
```

### Problema: Cache stale
```bash
rm -rf __pycache__ .pytest_cache
pytest --cache-clear tests/
```

---

## 10. Métricas Atuais (Janeiro 2026)

| Métrica | Status | Alvo |
|---------|--------|------|
| Coverage | 92% | >= 90% ✓ |
| Tests | Todos passam | 100% ✓ |
| Lint warnings | 0 | 0 ✓ |
| Type errors | 0 | 0 ✓ |
| Build time | ~2 min | < 5 min ✓ |

---

## 11. Referências

- [Pytest Documentation](https://docs.pytest.org/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [PyTorch Testing](https://pytorch.org/docs/stable/testing.html)
- [GitHub Actions](https://docs.github.com/en/actions)
- [PEP 8](https://www.python.org/dev/peps/pep-0008/)

---

**Última Atualização**: Janeiro 2026  
**Mantido por**: Equipe 5MLET  
**Status**: Ativo ✓
